{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Submission Template\n\nSo for submission this is how it is going to look:\n\n\n<code>ID Width Height Predicition_String</code>\n\nWhere Prediction string is:\n\n<code>0_class cell_0_class_prob cell_0_rle_binary_mask_encoded\n1_class cell_0_class_prob cell_0_rle_binary_mask_encoded\n2_class cell_0_class_prob cell_0_rle_binary_mask_encoded    \n3_class cell_0_class_prob cell_0_rle_binary_mask_encoded\n4_class cell_0_class_prob cell_0_rle_binary_mask_encoded\n5_class cell_0_class_prob cell_0_rle_binary_mask_encoded\n6_class cell_0_class_prob cell_0_rle_binary_mask_encoded    \n7_class cell_0_class_prob cell_0_rle_binary_mask_encoded     \n8_class cell_0_class_prob cell_0_rle_binary_mask_encoded \n9_class cell_0_class_prob cell_0_rle_binary_mask_encoded \n10_class cell_0_class_prob cell_0_rle_binary_mask_encoded     \n11_class cell_0_class_prob cell_0_rle_binary_mask_encoded    \n12_class cell_0_class_prob cell_0_rle_binary_mask_encoded     \n13_class cell_0_class_prob cell_0_rle_binary_mask_encoded     \n14_class cell_0_class_prob cell_0_rle_binary_mask_encoded     \n15_class cell_0_class_prob cell_0_rle_binary_mask_encoded     \n16_class cell_0_class_prob cell_0_rle_binary_mask_encoded \n17_class cell_0_class_prob cell_0_rle_binary_mask_encoded     \n18_class cell_0_class_prob cell_0_rle_binary_mask_encoded</code>     \n    \n<code>0_class cell_1_class_prob cell_1_rle_binary_mask_encoded\n1_class cell_1_class_prob cell_1_rle_binary_mask_encoded\n2_class cell_1_class_prob cell_1_rle_binary_mask_encoded    \n3_class cell_1_class_prob cell_1_rle_binary_mask_encoded\n4_class cell_1_class_prob cell_1_rle_binary_mask_encoded\n5_class cell_1_class_prob cell_1_rle_binary_mask_encoded\n6_class cell_1_class_prob cell_1_rle_binary_mask_encoded    \n7_class cell_1_class_prob cell_1_rle_binary_mask_encoded     \n8_class cell_1_class_prob cell_1_rle_binary_mask_encoded \n9_class cell_1_class_prob cell_1_rle_binary_mask_encoded \n10_class cell_1_class_prob cell_1_rle_binary_mask_encoded     \n11_class cell_1_class_prob cell_1_rle_binary_mask_encoded    \n12_class cell_1_class_prob cell_1_rle_binary_mask_encoded     \n13_class cell_1_class_prob cell_1_rle_binary_mask_encoded     \n14_class cell_1_class_prob cell_1_rle_binary_mask_encoded     \n15_class cell_1_class_prob cell_1_rle_binary_mask_encoded     \n16_class cell_1_class_prob cell_1_rle_binary_mask_encoded \n17_class cell_1_class_prob cell_1_rle_binary_mask_encoded     \n18_class cell_1_class_prob cell_1_rle_binary_mask_encoded</code>         \n    \n    \n...\n\n<code>0_class cell_n_class_prob cell_n_rle_binary_mask_encoded\n1_class cell_n_class_prob cell_n_rle_binary_mask_encoded\n2_class cell_n_class_prob cell_n_rle_binary_mask_encoded    \n3_class cell_n_class_prob cell_n_rle_binary_mask_encoded\n4_class cell_n_class_prob cell_n_rle_binary_mask_encoded\n5_class cell_n_class_prob cell_n_rle_binary_mask_encoded\n6_class cell_n_class_prob cell_n_rle_binary_mask_encoded    \n7_class cell_n_class_prob cell_n_rle_binary_mask_encoded     \n8_class cell_n_class_prob cell_n_rle_binary_mask_encoded \n9_class cell_n_class_prob cell_n_rle_binary_mask_encoded \n10_class cell_n_class_prob cell_n_rle_binary_mask_encoded     \n11_class cell_n_class_prob cell_n_rle_binary_mask_encoded    \n12_class cell_n_class_prob cell_n_rle_binary_mask_encoded     \n13_class cell_n_class_prob cell_n_rle_binary_mask_encoded     \n14_class cell_n_class_prob cell_n_rle_binary_mask_encoded     \n15_class cell_n_class_prob cell_n_rle_binary_mask_encoded     \n16_class cell_n_class_prob cell_n_rle_binary_mask_encoded \n17_class cell_n_class_prob cell_n_rle_binary_mask_encoded     \n18_class cell_n_class_prob cell_n_rle_binary_mask_encoded</code>     \n    \n    \n    \nSo as you can see, It is going to be pretty long.\n\nThe sample_submission.csv contains all the test folder ID's so all we need to do is predict on all of those images in the above format.","metadata":{"_kg_hide-input":false,"_kg_hide-output":false}},{"cell_type":"code","source":"tez_path = '../input/dyno-pytorch'\nimport sys\nsys.path.append(tez_path)","metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# For pixel normalisation\nMEAN_CHANNEL_VALUES=(0.485, 0.456, 0.406)\nCHANNEL_STD_DEV=(0.229, 0.224, 0.225)\n#MEAN_CHANNEL_VALUES = (0.07730, 0.05958, 0.07135)  # RGB\n#CHANNEL_STD_DEV = (0.12032, 0.08593, 0.14364)\n\n# Path to weights\nMODEL_PATH = '../input/models/model_split_0.bin'\nRESNET_PATH = '../input/models/resnet18.pth'","metadata":{"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"!pip install -q \"../input/pycocotools/pycocotools-2.0-cp37-cp37m-linux_x86_64.whl\"\n!pip install -q \"../input/hpapytorchzoozip/pytorch_zoo-master\"\n!pip install -q \"../input/hpacellsegmentatormaster/HPA-Cell-Segmentation-master\"","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# imports\nimport pandas as pd \nimport base64\nimport numpy as np\nfrom pycocotools import _mask as coco_mask\nfrom typing import List, Tuple\nimport zlib\n\nimport imageio\nimport matplotlib.pyplot as plt\nimport random\n\n# model\nimport torch\nfrom torch import nn\nfrom torchvision.models import resnet18\nimport tez\nimport albumentations as A\nimport numpy as np\nfrom skimage.transform import resize\n\n# HPA\nimport os\nimport imageio\nfrom hpacellseg.cellsegmentator import CellSegmentator\nfrom hpacellseg.utils import label_cell","metadata":{"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import hpacellseg.cellsegmentator as cellsegmentator\nfrom hpacellseg.utils import label_cell, label_nuclei\n\nNUC_MODEL = '../input/hpacellsegmentatormodelweights/dpn_unet_nuclei_v1.pth'\nCELL_MODEL = '../input/hpacellsegmentatormodelweights/dpn_unet_cell_3ch_v1.pth'\n\nsegmentator = cellsegmentator.CellSegmentator(\n    NUC_MODEL,\n    CELL_MODEL,\n    scale_factor=0.25,\n    device='cuda',\n    padding=False,\n    multi_channel_model=True\n)","metadata":{"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"please compile abn\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/serialization.py:658: SourceChangeWarning: source code of class 'pytorch_zoo.unet.DPNUnet' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n  warnings.warn(msg, SourceChangeWarning)\n/opt/conda/lib/python3.7/site-packages/torch/serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.container.ModuleList' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n  warnings.warn(msg, SourceChangeWarning)\n/opt/conda/lib/python3.7/site-packages/torch/serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n  warnings.warn(msg, SourceChangeWarning)\n/opt/conda/lib/python3.7/site-packages/torch/serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n  warnings.warn(msg, SourceChangeWarning)\n/opt/conda/lib/python3.7/site-packages/torch/serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n  warnings.warn(msg, SourceChangeWarning)\n/opt/conda/lib/python3.7/site-packages/torch/serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.upsampling.Upsample' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n  warnings.warn(msg, SourceChangeWarning)\n/opt/conda/lib/python3.7/site-packages/torch/serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n  warnings.warn(msg, SourceChangeWarning)\n/opt/conda/lib/python3.7/site-packages/torch/serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.MaxPool2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n  warnings.warn(msg, SourceChangeWarning)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# assigning bio style variables to images\n\n<code>mt = red_images # microtubules\ner = yellow_images # endoplasmic reticulum\nnu = blue_images # nucleus \npr = green_images # [proteins]\nimages = [mt, er, nu]</code>","metadata":{}},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"class ResNet18(tez.Model):\n    '''Model class to facilitate transfer learning \n    from a resnet-18 model\n    '''\n    NUM_CLASSES = 19\n    DROPOUT_RATE = 0.1\n    \n    def __init__(self, resnet, train_df=None, valid_df=None, batch_size=16, train_aug=None, valid_aug=None, pretrained=True):\n        # Initialise pretrained net and final layers for cell classification\n        super().__init__()\n        self.convolutions = nn.Sequential(*(list(resnet.children())[0:-1]))\n        self.dropout = nn.Dropout(self.DROPOUT_RATE)\n        self.dense = nn.Linear(512, self.NUM_CLASSES)\n        self.out = nn.Sigmoid()\n        self.loss_fn = nn.BCELoss()\n                \n    def forward(self, image, target=None):\n        batch_size = image.shape[0]\n        \n        # Extracts 512x1 feature vector from pretrained resnet18 conv layers\n        x = self.convolutions(image).reshape(batch_size, -1)\n        # Fully connected dense layer to 19 class output\n        output = self.dense(self.dropout(x))\n        # Sigmoid activations on output to infer class probabilities\n        output_probs = self.out(output)\n        \n        if target is not None:\n            loss = self.loss_fn(output_probs, target.to(torch.float32))  # why to float32???\n            metrics = self.monitor_metrics(output_probs, target)\n            return output_probs, loss, metrics\n        return output_probs, None, None\n    \n    def fetch_optimizer(self):\n        opt = torch.optim.Adam(self.parameters(), lr=3e-4)\n        return opt\n    \n    def fetch_scheduler(self):\n        sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n            self.optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1\n        )\n        return sch\n\ninf_aug = A.Compose([\n    A.Normalize(\n        mean=(0.485, 0.456, 0.406), \n        std=(0.229, 0.224, 0.225), \n        max_pixel_value=1.0,\n        p=1.0\n    )\n])    ","metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def infer(img, model, inf_aug):\n    ''' In: np.array (512, 512, 3) image\n        Out: np.array (1, 19) label-probs\n    '''\n    # Augment; re-shuffle channels; reshape; send to gpu\n    X = inf_aug(image=img)['image']\n    X = np.transpose(img, (2, 0, 1)).astype(np.float32)\n    X = torch.tensor(X, dtype=torch.float32).to('cuda')\n    X = X.unsqueeze(0)\n    with torch.no_grad():\n        out = model(X)[0]\n    return out.cpu().detach().numpy()\n","metadata":{"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Helper Functions","metadata":{}},{"cell_type":"code","source":"def get_inference(rgby_channel: tuple, model, inf_aug) -> tuple:\n    \"\"\"\n    Input: rgby_channel for cell\n    Pass: pass on model and augmentations for inference\n    Output: prediction from inference of model\n    \n    NB: This function currently randomly predicts each class\n    \"\"\"\n    # unpack rgby channel\n    r, g, b, y = rgby_channel\n    # stack img array\n    img = np.dstack((r, g, b))\n    # resize like in training\n    img = resize(img, (512, 512))\n    # get inference\n    pred = infer(img, model, inf_aug)\n    # ensure right shape\n    assert pred.shape[0]==1\n    assert pred.shape[1]==19\n    return(pred[0])\n\n\ndef convert_sample_submission(path='/kaggle/input/hpa-single-cell-image-classification/sample_submission.csv')-> pd.core.frame.DataFrame:\n    \"\"\"\n    Input: path to sample_submission.csv\n    Output: csv submission without PredictionString\n    \"\"\"\n    # read sample_submission csv\n    sample_submission = pd.read_csv(path)\n    # create new dataframe without prediction String\n    submission = sample_submission.drop(columns = ['PredictionString'])\n    return(submission)\n    \n    \ndef read_image_array(ID, path='/kaggle/input/hpa-single-cell-image-classification/test/'):\n    \"\"\"\n    Input: ID and path to images\n    Output: np arrays for each channels\n    \"\"\"\n    r = imageio.imread(path+ID+'_red.png')\n    g = imageio.imread(path+ID+'_green.png')\n    b = imageio.imread(path+ID+'_blue.png')\n    y = imageio.imread(path+ID+'_yellow.png')\n    return r, g, b, y\n\n\ndef segment(ref_channels: List[str], segmentator: CellSegmentator)-> np.ndarray:\n    \"\"\"\n    Input: reference channels for segementation class and segmentation class\n    Output: mask for image\n    \"\"\"\n    # Ref channels must be in order red, yellow, blue\n    input_ = [[i] for i in ref_channels]  # Segmentator only accepts list of lists input\n    nuc_segmentation = segmentator.pred_nuclei(input_[2])[0]\n    cell_segmentation = segmentator.pred_cells(input_)[0]\n    mask = label_cell(nuc_segmentation, cell_segmentation)[1]\n    return mask\n\n\ndef get_rle_string(rgby_channel: tuple, binary_mask: np.ndarray, model, inf_aug)-> str:\n    \"\"\"\n    Input: cell rgby channels and binary mask for cell\n    Pass: pass on model and augmentations for inference\n    Output: prediction string for cell\n    \"\"\"\n    # get predictions\n    class_preds = get_inference(rgby_channel, model, inf_aug)\n    # add to prediction string\n    enc = encode_binary_mask(binary_mask)\n    # pred string\n    pred_string = ''\n    for jj, pred in enumerate(class_preds):\n        pred_string += str(jj)+' '+str(pred)+' '+enc+ ' '\n    return(pred_string)\n\n\ndef extract_and_predict(channels: tuple, mask: np.ndarray, model, inf_aug) -> str:\n    \"\"\"\n    Input: whole image channels and mask for whole image\n    Pass: passing on model and augmentations for inference\n    Output: prediction string for whole image to be inputted to csv\n    \"\"\"\n    # master predicted_string\n    predicted_string = ''\n    for label in np.unique(mask):\n        # Get values from where image == label\n        if label == 0:\n            continue  # ignore background\n        temp_mask = mask.copy()\n        temp_mask[temp_mask != label] = 0\n        temp_mask[temp_mask == label] = 1\n        # Get temp mask bounding box coords\n        idxs = np.asarray(temp_mask == 1).nonzero()\n        y_min, y_max = idxs[0].min(), idxs[0].max()\n        x_min, x_max = idxs[1].min(), idxs[1].max()\n\n        # tuple of all channels to be classified\n        rgby_channel_list = list()\n        for channel in channels:\n            # Zero pad and square off\n            single_cell = temp_mask * channel\n            single_cell = single_cell[y_min:(y_max + 1), x_min:(x_max + 1)]\n            # append channel to list\n            rgby_channel_list.append(single_cell)\n        # return prediction string to append\n        predicted_string += get_rle_string(tuple(rgby_channel_list), np.array(temp_mask, dtype=bool),\n                                           model, inf_aug)\n    return(predicted_string)\n\n\ndef encode_binary_mask(mask: np.ndarray)-> str:\n    \"\"\"\n    Input: binary mask\n    Output: encoded rle mask, decoded into ascii text\n    \"\"\"\n\n    # check input mask --\n    if mask.dtype != np.bool:\n        raise ValueError(\n           \"encode_binary_mask expects a binary mask, received dtype == %s\" %\n           mask.dtype)\n\n    mask = np.squeeze(mask)\n    if len(mask.shape) != 2:\n        raise ValueError(\n           \"encode_binary_mask expects a 2d mask, received shape == %s\" %\n           mask.shape)\n\n    # convert input mask to expected COCO API input --\n    mask_to_encode = mask.reshape(mask.shape[0], mask.shape[1], 1)\n    mask_to_encode = mask_to_encode.astype(np.uint8)\n    mask_to_encode = np.asfortranarray(mask_to_encode)\n\n    # RLE encode mask --\n    encoded_mask = coco_mask.encode(mask_to_encode)[0][\"counts\"]\n\n    # compress and base64 encoding --\n    binary_str = zlib.compress(encoded_mask, zlib.Z_BEST_COMPRESSION)\n    base64_str = base64.b64encode(binary_str)\n    return base64_str.decode('ascii')","metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"# load data\ndf = convert_sample_submission()\n# load pretrained model\nresnet_model = resnet18(pretrained=False)\nresnet_model.load_state_dict(torch.load(RESNET_PATH))\n# define model class\nmodel = ResNet18(resnet_model)\n# load weights\nmodel.load(MODEL_PATH)\n# augmentations\ninf_aug = A.Compose([\nA.Normalize(\n    mean=MEAN_CHANNEL_VALUES, \n    std=CHANNEL_STD_DEV, \n    max_pixel_value=1.0,\n    p=1.0\n    )\n])   ","metadata":{"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Iterate over test dataframce\nfor image in df.itertuples():\n    print('#####################################################')\n    print('Segmenting image {}'.format(image.ID))\n    try:    \n        # segment and extract\n        r, g, b, y = read_image_array(image.ID)\n        # assign ref channels\n        ref_channels = [r, y, b]\n        # get mask\n        mask = segment(ref_channels, segmentator)\n        # get predicted string\n        df.at[image.Index, 'PredictedString'] = extract_and_predict((r, g, b, y), \n                                                                    mask,\n                                                                    model,\n                                                                    inf_aug)\n    except:\n        continue\n\n#     # plot \n#     fig, ax = plt.subplots(figsize=(8, 8))\n#     ax.imshow(np.dstack((r, g, b)))\n#     ax.imshow(mask, alpha=0.3)\n#     plt.title('Image Example: {}'.format(ID))\n#     plt.show()\n#     print(predicted_string)","metadata":{"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"#####################################################\nSegmenting image 0040581b-f1f2-4fbe-b043-b6bfea5404bb\n#####################################################\nSegmenting image 004a270d-34a2-4d60-bbe4-365fca868193\n#####################################################\nSegmenting image 00537262-883c-4b37-a3a1-a4931b6faea5\n#####################################################\nSegmenting image 00c9a1c9-2f06-476f-8b0d-6d01032874a2\n#####################################################\nSegmenting image 0173029a-161d-40ef-af28-2342915b22fb\n#####################################################\nSegmenting image 01a14326-67b8-43b0-ac7a-ba6dfb3c38ad\n#####################################################\nSegmenting image 020a29cf-2c24-478b-8603-c22a90dc3e31\n#####################################################\nSegmenting image 02425037-3048-4ff3-9c0a-e9fc2d033e32\n#####################################################\nSegmenting image 02531b54-078d-4a33-9c9c-0632b58c0a9a\n#####################################################\nSegmenting image 025f04f1-6c68-4606-b34b-047738dd4804\n#####################################################\nSegmenting image 02861579-97fb-4482-a613-356e0bf5d090\n#####################################################\nSegmenting image 02b3c5aa-d70c-49d1-b3e5-3f5cc10375ca\n#####################################################\nSegmenting image 02f9ee97-bf2e-4d9d-bdfd-b903ec2e79c0\n#####################################################\nSegmenting image 036cd088-40fc-4ab0-a131-a6c02f02440a\n#####################################################\nSegmenting image 03aa8433-b927-4495-a6e2-d03cdc17d76b\n#####################################################\nSegmenting image 04c1b106-a5a4-419d-96c6-2750ebe2b50d\n","output_type":"stream"}]},{"cell_type":"code","source":"# how many images were unsucessful\ndf[df.PredictedString.isnull()==True]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission cv\ndf.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}