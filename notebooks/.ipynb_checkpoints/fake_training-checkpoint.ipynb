{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cab3826",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "\n",
    "For our problem we have the output from the model as:\n",
    "\n",
    "$[N, C]$\n",
    "\n",
    "And our labels are one hot encoding are the same:\n",
    "\n",
    "$[N, C]$ \n",
    "\n",
    "where N is the number of samples, C is either {1, 0}.\n",
    "\n",
    "In PyTorch torch.nn.BCELoss() is [Binary Cross Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html):\n",
    "\n",
    "Which expects the input:\n",
    "\n",
    "***\n",
    "\n",
    "Input: (N, *)(N,∗) where *∗ means, any number of additional dimensions\n",
    "\n",
    "Target: (N, *)(N,∗) , same shape as the input\n",
    "\n",
    "Output: scalar. If reduction is 'none', then (N, *)(N,∗) , same shape as input.\n",
    "\n",
    "***\n",
    "\n",
    "We want to apply the sigmoid function to the inputs to ensure they are in the 0 -> 1 range.\n",
    "\n",
    "We could also just use nn.BCE() which includes this.\n",
    "\n",
    "[A good example](https://jbencook.com/cross-entropy-loss-in-pytorch/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6944d10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import resnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae00ad9f",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39ffff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00770ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct += torch.sum((pred >= 0.5).float() == y.float()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5f7c1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65563d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct / (1*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3a34fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake ouputs from model and target\n",
    "pred2 = torch.tensor([[0.5, 0., 1.]])\n",
    "y2 = torch.tensor([[1., 1., 0.]])\n",
    "correct += torch.sum((pred2 >= 0.5).float() == y2.float()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45a19848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct / (2*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f70ce85",
   "metadata": {},
   "source": [
    "# Fake Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "26a008f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, NUM_CLASSES, DROPOUT_RATE):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.convolutions = nn.Sequential(*(list(resnet18().children())[0:-1]))\n",
    "        self.dropout = nn.Dropout(DROPOUT_RATE)\n",
    "        self.dense = nn.Linear(512, NUM_CLASSES)\n",
    "        self.out = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, X):\n",
    "        batch_size = X.shape[0]\n",
    "        # Extracts 512x1 feature vector from pretrained resnet18 conv layers\n",
    "        x = self.convolutions(X).reshape(batch_size, -1)\n",
    "        # Fully connected dense layer to 19 class output\n",
    "        output = self.dense(self.dropout(x))\n",
    "        # Sigmoid activations on output to infer class probabilities\n",
    "        output_probs = self.out(output)\n",
    "        return output_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f7623619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 19])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork(19, 0.1)\n",
    "X = torch.randn([5, 3, 512, 512])\n",
    "\n",
    "pred = model(X)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4ef74ad2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6489, 0.5964, 0.3269, 0.5840, 0.6586, 0.4613, 0.4889, 0.4423, 0.5369,\n",
       "         0.6509, 0.4612, 0.5653, 0.2981, 0.5687, 0.5420, 0.5841, 0.3994, 0.5663,\n",
       "         0.4315],\n",
       "        [0.6424, 0.5848, 0.4159, 0.5126, 0.6514, 0.4954, 0.4241, 0.3870, 0.5765,\n",
       "         0.5841, 0.4682, 0.5380, 0.3206, 0.5519, 0.5673, 0.6103, 0.3598, 0.4929,\n",
       "         0.4171],\n",
       "        [0.6688, 0.5651, 0.3660, 0.5471, 0.6974, 0.3379, 0.4272, 0.4477, 0.6643,\n",
       "         0.5606, 0.4750, 0.5422, 0.3074, 0.5331, 0.5420, 0.5435, 0.3296, 0.5959,\n",
       "         0.4321],\n",
       "        [0.6132, 0.6145, 0.3807, 0.5609, 0.6192, 0.4404, 0.4685, 0.4996, 0.6604,\n",
       "         0.5599, 0.4437, 0.5957, 0.2183, 0.4819, 0.4476, 0.5802, 0.3807, 0.5231,\n",
       "         0.4161],\n",
       "        [0.6484, 0.6554, 0.3867, 0.5343, 0.5818, 0.3799, 0.5081, 0.3948, 0.6358,\n",
       "         0.6064, 0.4788, 0.5642, 0.2464, 0.5235, 0.5132, 0.5608, 0.3935, 0.5355,\n",
       "         0.4068]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961bfdde",
   "metadata": {},
   "source": [
    "# Fake Loss Function\n",
    "Using fake model, let's get the fake loss function working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5acadc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "684eb343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 19])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we have to make fake Ground Truths\n",
    "ground_truth = torch.full((5, 19), 1)\n",
    "ground_truth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4c891ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8279be16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7060, grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute loss \n",
    "loss = loss_fn(pred, ground_truth.float())\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd4d3cb",
   "metadata": {},
   "source": [
    "We then can use loss.backward() to update the change in loss for the weights.\n",
    "\n",
    "<code>loss.backward() computes dloss/dx for every parameter x which has requires_grad=True. These are accumulated into x.grad for every parameter x. In pseudo-code:\n",
    "\n",
    "x.grad += dloss/dx\n",
    "optimizer.step updates the value of x using the gradient x.grad. For example, the SGD optimizer performs:\n",
    "x += -lr * x.grad\n",
    "optimizer.zero_grad() clears x.grad for every parameter x in the optimizer. It’s important to call this before loss.backward(), otherwise you’ll accumulate the gradients from multiple passes.</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8f9eba",
   "metadata": {},
   "source": [
    "# Fake Optimiser\n",
    "\n",
    "N.B In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes. This is convenient while training RNNs. So, the default action is to accumulate (i.e. sum) the gradients on every loss.backward() call.\n",
    "\n",
    "Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly. Else the gradient would point in some other direction than the intended direction towards the minimum (or maximum, in case of maximization objectives).\n",
    "\n",
    "[Why Choose Adam](https://debuggercafe.com/adam-algorithm-for-deep-learning-optimization/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b9c588cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.0001) # we could put a scheduler here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a26a51b",
   "metadata": {},
   "source": [
    "When we use te optimizer what we are doing is [because](https://deeplearningdemystified.com/article/fdl-4) we want to avoid certain traps in achieving minimal loss (local optima, changing how certain weights are updated).\n",
    "\n",
    "To use the optimiser we first initialize (as above) and and then in the training loop:\n",
    "\n",
    "<code>#Backpropagation\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2627a03",
   "metadata": {},
   "source": [
    "# Fake Training Loop\n",
    "Using fake inputs and predicitons I am going to show what the training loop should look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a7963952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, optim, dataloader, loss_fn, USE_GPU=False):\n",
    "    # How Long is our dataset\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    # Firstly set model to training\n",
    "    model.train()\n",
    "    # if we are using a GPU send the model to device\n",
    "    if USE_GPU:\n",
    "        model = model.cuda()\n",
    "    \n",
    "    # Logging and Stats\n",
    "    loss_log = list()\n",
    "    \n",
    "    # Set gradients to be trainable\n",
    "    with torch.set_grad_enabled(True):\n",
    "        for batch_num, (X,y) in enumerate(dataloader):\n",
    "            # Compute prediction and loss\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred, y)\n",
    "            \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Logging and Stats\n",
    "            if batch_num % 100 == 0:\n",
    "                loss, current = loss.item(), batch * len(X)\n",
    "                loss_log.append(loss)\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "                \n",
    "    return(loss_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b12b3c0",
   "metadata": {},
   "source": [
    "# Fake Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d742c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(model, optim, dataloader, loss_fn, USE_GPU=False):\n",
    "    # How long is our dataset\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
