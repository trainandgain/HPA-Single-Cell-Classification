{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "609d5f3d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4a35cc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "\n",
    "# transformers\n",
    "from torchvision import transforms\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "import albumentations\n",
    "\n",
    "# dataset imports\n",
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# evaluation imports\n",
    "import time\n",
    "from sklearn import metrics\n",
    "\n",
    "# model\n",
    "import copy\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from sklearn.metrics import average_precision_score\n",
    "import tez\n",
    "from tez.callbacks import EarlyStopping\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a9a3da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNELS = ['red', 'green', 'blue', 'yellow']\n",
    "TRAIN_CSV = '../input/image_subset/cell/train.csv'\n",
    "IMG_DIR = '../input/image_subset/cell/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f03a05",
   "metadata": {},
   "source": [
    "# Dataset Class\n",
    "\n",
    "Each Image has already been pre segmented, we will then split into n number of folds and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f66afd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellDataset(object):\n",
    "    '''Dataset class to fetch HPA cell-level images\n",
    "    and corresponding weak labels\n",
    "    '''\n",
    "    def __init__(self, images, targets, img_root, augmentations=None):\n",
    "        self.images = images\n",
    "        self.targets = targets\n",
    "        self.img_root = img_root\n",
    "        self.augmentations = augmentations\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.images[idx] \n",
    "        img_channels = self._fetch_channels(img_id)\n",
    "        img = self._channels_2_array(img_channels)\n",
    "        print(img.shape)\n",
    "        img = resize(img, (512, 512))  # Always resize cell images for collate function\n",
    "        # If augmentation pipeline provided, apply augmentations\n",
    "        if self.augmentations:\n",
    "            img = self.augmentations(image=img)['image']\n",
    "        # Adjust to channel first indexing for pytorch (speed reasons)\n",
    "        features = np.transpose(img, (2, 0, 1)).astype(np.float32)\n",
    "        target = self.targets[idx]  # Grab target vector\n",
    "        \n",
    "        return {'image': torch.tensor(features),\n",
    "                'target': torch.tensor(target)}\n",
    "    \n",
    "    def _fetch_channels(self, img_id: str, channel_names=CHANNELS):\n",
    "        'Return absolute path of segmentation channels of a given image id'\n",
    "        base = os.path.join(self.img_root, img_id)\n",
    "        return [base + '_' + i  + '.png' for i in channel_names]\n",
    "                                         \n",
    "    def _channels_2_array(self, img_channels):\n",
    "        'Return 3D array of pixel values of input image channels'\n",
    "        print(img_channels)\n",
    "        r = plt.imread(img_channels[0])\n",
    "        g = plt.imread(img_channels[1])\n",
    "        b = plt.imread(img_channels[2])\n",
    "        pixel_arr = np.dstack((r, g, b))\n",
    "        return pixel_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9d78cc",
   "metadata": {},
   "source": [
    "# Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94484da2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ResNet18(tez.Model):\n",
    "    '''Model class to facilitate transfer learning \n",
    "    from a resnet-18 model\n",
    "    '''\n",
    "    NUM_CLASSES = 19\n",
    "    IMG_DIR = '../input/image_subset/cell/'\n",
    "    DROPOUT_RATE = 0.1\n",
    "    \n",
    "    def __init__(self, train_dl, valid_dl, batch_size=16, pretrained=True):\n",
    "        # Initialise pretrained net and final layers for cell classification\n",
    "        super().__init__()\n",
    "        self.convolutions = nn.Sequential(*(list(resnet18(pretrained).children())[0:-1]))\n",
    "        self.dropout = nn.Dropout(self.DROPOUT_RATE)\n",
    "        self.dense = nn.Linear(512, self.NUM_CLASSES)\n",
    "        self.out = nn.Sigmoid()\n",
    "        self.loss_fn = nn.BCELoss()\n",
    "        \n",
    "        # Below should probably be in tez.Model super class but is a quick hack around\n",
    "        # Training time image augmentation stack\n",
    "        self.train_loader = train_dl\n",
    "        self.valid_loader = valid_dl\n",
    "        \n",
    "    def forward(self, image, target=None):\n",
    "        batch_size = image.shape[0]\n",
    "        \n",
    "        # Extracts 512x1 feature vector from pretrained resnet18 conv layers\n",
    "        x = self.convolutions(image).reshape(batch_size, -1)\n",
    "        # Fully connected dense layer to 19 class output\n",
    "        output = self.dense(self.dropout(x))\n",
    "        # Sigmoid activations on output to infer class probabilities\n",
    "        output_probs = self.out(output)\n",
    "        \n",
    "        if target is not None:\n",
    "            loss = self.loss_fn(output_probs, target.to(torch.float32))  # why to float32???\n",
    "            metrics = self.monitor_metrics(output_probs, target)\n",
    "            return output_probs, loss, metrics\n",
    "        return output_probs, None, None\n",
    "    \n",
    "    def monitor_metrics(self, outputs, targets):\n",
    "        if targets is None:\n",
    "            return {}\n",
    "        targets = targets.cpu().detach().numpy()\n",
    "        outputs = outputs.cpu().detach().numpy()\n",
    "        precision = average_precision_score(targets, outputs, average=None)\n",
    "        #precision = accuracy_score(targets, outputs)\n",
    "        return {\"precision\": precision}\n",
    "    \n",
    "    def fetch_optimizer(self):\n",
    "        opt = torch.optim.Adam(self.parameters(), lr=3e-4)\n",
    "        return opt\n",
    "    \n",
    "    def fetch_scheduler(self):\n",
    "        sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            self.optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1\n",
    "        )\n",
    "        return sch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea85d93",
   "metadata": {},
   "source": [
    "# Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ebe64d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image augmentation stack \n",
    "train_aug = albumentations.Compose([\n",
    "    albumentations.augmentations.transforms.Normalize(\n",
    "        mean=(0.485, 0.456, 0.406), \n",
    "        std=(0.229, 0.224, 0.225), \n",
    "        max_pixel_value=255.0\n",
    "    ),\n",
    "    albumentations.Transpose(p=0.5),\n",
    "    albumentations.HorizontalFlip(p=0.5),\n",
    "    albumentations.VerticalFlip(p=0.5),\n",
    "])\n",
    "\n",
    "valid_aug = albumentations.Compose([\n",
    "    albumentations.augmentations.transforms.Normalize(\n",
    "        mean=(0.485, 0.456, 0.406), \n",
    "        std=(0.229, 0.224, 0.225), \n",
    "        max_pixel_value=255.0\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afe6832",
   "metadata": {},
   "source": [
    "# Stratified-K-Folding and Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2e08600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_split_df_cell(df, nfolds=2, order=2):\n",
    "    # deep copy so changes can propogate\n",
    "    df_copy = copy.deepcopy(df)\n",
    "    # define label rows\n",
    "    labels = [str(i) for i in range(19)]\n",
    "        # add OHE columns\n",
    "    for i in range(19):\n",
    "        # Label column contains string not np.array\n",
    "        df_copy['{}'.format(i)] = df.Label.apply(lambda x: (int(x.strip('[]').replace(', ', '')[i])))\n",
    "        \n",
    "    df_copy = df_copy.set_index(\"cell_id\")\n",
    "    \n",
    "    split_df = df_copy.iloc[:][labels]\n",
    "    \n",
    "    split_df = split_df.groupby(split_df.index).sum() \n",
    "\n",
    "    X, y = split_df.index.values, split_df.values\n",
    "\n",
    "    k_fold = IterativeStratification(n_splits=nfolds, order=order)\n",
    "\n",
    "    splits = list(k_fold.split(X, y))\n",
    "\n",
    "    fold_splits = np.zeros(df.shape[0]).astype(np.int32)\n",
    "\n",
    "    for i in range(nfolds):\n",
    "        fold_splits[splits[i][1]] = i\n",
    "\n",
    "    split_df['Split'] = fold_splits    \n",
    "\n",
    "    df_folds = []\n",
    "\n",
    "    for fold in range(nfolds):\n",
    "\n",
    "        df_fold = split_df.copy()\n",
    "            \n",
    "        train_df = df_fold[df_fold.Split != fold].drop('Split', axis=1).reset_index()\n",
    "        \n",
    "        val_df = df_fold[df_fold.Split == fold].drop('Split', axis=1).reset_index()\n",
    "        \n",
    "        df_folds.append((train_df, val_df))\n",
    "\n",
    "    return df_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "817b1341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_dataloaders(split, batch_size, train_aug=None, valid_aug=None):\n",
    "    labels = [str(i) for i in range(19)]\n",
    "    train_df, val_df = split\n",
    "    # dataset class with augmentations\n",
    "    train_ds = CellDataset(train_df.cell_id.values, \n",
    "                           np.array(train_df.loc[:, labels]),\n",
    "                           IMG_DIR,\n",
    "                           augmentations=train_aug)\n",
    "    \n",
    "    val_ds = CellDataset(val_df.cell_id.values, \n",
    "                         np.array(val_df.loc[:, labels]),\n",
    "                         IMG_DIR,\n",
    "                         augmentations=valid_aug)\n",
    "    \n",
    "    # dataloaders for each split\n",
    "    train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "    val_dl = DataLoader(val_ds, batch_size)\n",
    "    # return splits dataloaders\n",
    "    return train_dl, val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ea02ece",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV and devising folds...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print('Reading CSV and devising folds...')\n",
    "# read training csv\n",
    "df = pd.read_csv(TRAIN_CSV)\n",
    "# get stratified k fold splits\n",
    "splits = create_split_df_cell(df, 2, order=2)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7277713a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                    | 0/4862 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../input/image_subset/cell/3c9e5634-bbae-11e8-b2ba-ac1f6b6435d0_cell_15_red.png', '../input/image_subset/cell/3c9e5634-bbae-11e8-b2ba-ac1f6b6435d0_cell_15_green.png', '../input/image_subset/cell/3c9e5634-bbae-11e8-b2ba-ac1f6b6435d0_cell_15_blue.png', '../input/image_subset/cell/3c9e5634-bbae-11e8-b2ba-ac1f6b6435d0_cell_15_yellow.png']\n",
      "(503, 420, 12)\n",
      "['../input/image_subset/cell/51943f28-bbb7-11e8-b2ba-ac1f6b6435d0_cell_11_red.png', '../input/image_subset/cell/51943f28-bbb7-11e8-b2ba-ac1f6b6435d0_cell_11_green.png', '../input/image_subset/cell/51943f28-bbb7-11e8-b2ba-ac1f6b6435d0_cell_11_blue.png', '../input/image_subset/cell/51943f28-bbb7-11e8-b2ba-ac1f6b6435d0_cell_11_yellow.png']\n",
      "(482, 495, 12)\n",
      "['../input/image_subset/cell/e8d0ae48-bba9-11e8-b2ba-ac1f6b6435d0_cell_10_red.png', '../input/image_subset/cell/e8d0ae48-bba9-11e8-b2ba-ac1f6b6435d0_cell_10_green.png', '../input/image_subset/cell/e8d0ae48-bba9-11e8-b2ba-ac1f6b6435d0_cell_10_blue.png', '../input/image_subset/cell/e8d0ae48-bba9-11e8-b2ba-ac1f6b6435d0_cell_10_yellow.png']\n",
      "(552, 286, 12)\n",
      "['../input/image_subset/cell/a385f7a8-bbae-11e8-b2ba-ac1f6b6435d0_cell_2_red.png', '../input/image_subset/cell/a385f7a8-bbae-11e8-b2ba-ac1f6b6435d0_cell_2_green.png', '../input/image_subset/cell/a385f7a8-bbae-11e8-b2ba-ac1f6b6435d0_cell_2_blue.png', '../input/image_subset/cell/a385f7a8-bbae-11e8-b2ba-ac1f6b6435d0_cell_2_yellow.png']\n",
      "(538, 142, 12)\n",
      "['../input/image_subset/cell/26d33e16-bbb1-11e8-b2ba-ac1f6b6435d0_cell_35_red.png', '../input/image_subset/cell/26d33e16-bbb1-11e8-b2ba-ac1f6b6435d0_cell_35_green.png', '../input/image_subset/cell/26d33e16-bbb1-11e8-b2ba-ac1f6b6435d0_cell_35_blue.png', '../input/image_subset/cell/26d33e16-bbb1-11e8-b2ba-ac1f6b6435d0_cell_35_yellow.png']\n",
      "(390, 308, 12)\n",
      "['../input/image_subset/cell/8d229e72-bba3-11e8-b2b9-ac1f6b6435d0_cell_4_red.png', '../input/image_subset/cell/8d229e72-bba3-11e8-b2b9-ac1f6b6435d0_cell_4_green.png', '../input/image_subset/cell/8d229e72-bba3-11e8-b2b9-ac1f6b6435d0_cell_4_blue.png', '../input/image_subset/cell/8d229e72-bba3-11e8-b2b9-ac1f6b6435d0_cell_4_yellow.png']\n",
      "(542, 496, 12)\n",
      "['../input/image_subset/cell/be5d951c-bbaf-11e8-b2ba-ac1f6b6435d0_cell_2_red.png', '../input/image_subset/cell/be5d951c-bbaf-11e8-b2ba-ac1f6b6435d0_cell_2_green.png', '../input/image_subset/cell/be5d951c-bbaf-11e8-b2ba-ac1f6b6435d0_cell_2_blue.png', '../input/image_subset/cell/be5d951c-bbaf-11e8-b2ba-ac1f6b6435d0_cell_2_yellow.png']\n",
      "(397, 458, 12)\n",
      "['../input/image_subset/cell/0d43ae1e-bba1-11e8-b2b9-ac1f6b6435d0_cell_33_red.png', '../input/image_subset/cell/0d43ae1e-bba1-11e8-b2b9-ac1f6b6435d0_cell_33_green.png', '../input/image_subset/cell/0d43ae1e-bba1-11e8-b2b9-ac1f6b6435d0_cell_33_blue.png', '../input/image_subset/cell/0d43ae1e-bba1-11e8-b2b9-ac1f6b6435d0_cell_33_yellow.png']\n",
      "(418, 541, 12)\n",
      "['../input/image_subset/cell/fa750eac-bba7-11e8-b2ba-ac1f6b6435d0_cell_15_red.png', '../input/image_subset/cell/fa750eac-bba7-11e8-b2ba-ac1f6b6435d0_cell_15_green.png', '../input/image_subset/cell/fa750eac-bba7-11e8-b2ba-ac1f6b6435d0_cell_15_blue.png', '../input/image_subset/cell/fa750eac-bba7-11e8-b2ba-ac1f6b6435d0_cell_15_yellow.png']\n",
      "(539, 550, 12)\n",
      "['../input/image_subset/cell/01314a58-bba3-11e8-b2b9-ac1f6b6435d0_cell_14_red.png', '../input/image_subset/cell/01314a58-bba3-11e8-b2b9-ac1f6b6435d0_cell_14_green.png', '../input/image_subset/cell/01314a58-bba3-11e8-b2b9-ac1f6b6435d0_cell_14_blue.png', '../input/image_subset/cell/01314a58-bba3-11e8-b2b9-ac1f6b6435d0_cell_14_yellow.png']\n",
      "(486, 543, 12)\n",
      "['../input/image_subset/cell/96bfd502-bb9a-11e8-b2b9-ac1f6b6435d0_cell_26_red.png', '../input/image_subset/cell/96bfd502-bb9a-11e8-b2b9-ac1f6b6435d0_cell_26_green.png', '../input/image_subset/cell/96bfd502-bb9a-11e8-b2b9-ac1f6b6435d0_cell_26_blue.png', '../input/image_subset/cell/96bfd502-bb9a-11e8-b2b9-ac1f6b6435d0_cell_26_yellow.png']\n",
      "(451, 309, 12)\n",
      "['../input/image_subset/cell/32a809d0-bbaa-11e8-b2ba-ac1f6b6435d0_cell_21_red.png', '../input/image_subset/cell/32a809d0-bbaa-11e8-b2ba-ac1f6b6435d0_cell_21_green.png', '../input/image_subset/cell/32a809d0-bbaa-11e8-b2ba-ac1f6b6435d0_cell_21_blue.png', '../input/image_subset/cell/32a809d0-bbaa-11e8-b2ba-ac1f6b6435d0_cell_21_yellow.png']\n",
      "(639, 632, 12)\n",
      "['../input/image_subset/cell/6b4734c6-bb9d-11e8-b2b9-ac1f6b6435d0_cell_8_red.png', '../input/image_subset/cell/6b4734c6-bb9d-11e8-b2b9-ac1f6b6435d0_cell_8_green.png', '../input/image_subset/cell/6b4734c6-bb9d-11e8-b2b9-ac1f6b6435d0_cell_8_blue.png', '../input/image_subset/cell/6b4734c6-bb9d-11e8-b2b9-ac1f6b6435d0_cell_8_yellow.png']\n",
      "(974, 652, 12)\n",
      "['../input/image_subset/cell/f91f3d68-bbaa-11e8-b2ba-ac1f6b6435d0_cell_39_red.png', '../input/image_subset/cell/f91f3d68-bbaa-11e8-b2ba-ac1f6b6435d0_cell_39_green.png', '../input/image_subset/cell/f91f3d68-bbaa-11e8-b2ba-ac1f6b6435d0_cell_39_blue.png', '../input/image_subset/cell/f91f3d68-bbaa-11e8-b2ba-ac1f6b6435d0_cell_39_yellow.png']\n",
      "(361, 250, 12)\n",
      "['../input/image_subset/cell/bb07875e-bba2-11e8-b2b9-ac1f6b6435d0_cell_3_red.png', '../input/image_subset/cell/bb07875e-bba2-11e8-b2b9-ac1f6b6435d0_cell_3_green.png', '../input/image_subset/cell/bb07875e-bba2-11e8-b2b9-ac1f6b6435d0_cell_3_blue.png', '../input/image_subset/cell/bb07875e-bba2-11e8-b2b9-ac1f6b6435d0_cell_3_yellow.png']\n",
      "(320, 240, 12)\n",
      "['../input/image_subset/cell/b68bc9a4-bbb8-11e8-b2ba-ac1f6b6435d0_cell_29_red.png', '../input/image_subset/cell/b68bc9a4-bbb8-11e8-b2ba-ac1f6b6435d0_cell_29_green.png', '../input/image_subset/cell/b68bc9a4-bbb8-11e8-b2ba-ac1f6b6435d0_cell_29_blue.png', '../input/image_subset/cell/b68bc9a4-bbb8-11e8-b2ba-ac1f6b6435d0_cell_29_yellow.png']\n",
      "(302, 278, 12)\n",
      "['../input/image_subset/cell/bb829160-bba7-11e8-b2ba-ac1f6b6435d0_cell_17_red.png', '../input/image_subset/cell/bb829160-bba7-11e8-b2ba-ac1f6b6435d0_cell_17_green.png', '../input/image_subset/cell/bb829160-bba7-11e8-b2ba-ac1f6b6435d0_cell_17_blue.png', '../input/image_subset/cell/bb829160-bba7-11e8-b2ba-ac1f6b6435d0_cell_17_yellow.png']\n",
      "(512, 408, 12)\n",
      "['../input/image_subset/cell/977b90d6-bbb2-11e8-b2ba-ac1f6b6435d0_cell_6_red.png', '../input/image_subset/cell/977b90d6-bbb2-11e8-b2ba-ac1f6b6435d0_cell_6_green.png', '../input/image_subset/cell/977b90d6-bbb2-11e8-b2ba-ac1f6b6435d0_cell_6_blue.png', '../input/image_subset/cell/977b90d6-bbb2-11e8-b2ba-ac1f6b6435d0_cell_6_yellow.png']\n",
      "(431, 429, 12)\n",
      "['../input/image_subset/cell/d9fb1038-bba0-11e8-b2b9-ac1f6b6435d0_cell_14_red.png', '../input/image_subset/cell/d9fb1038-bba0-11e8-b2b9-ac1f6b6435d0_cell_14_green.png', '../input/image_subset/cell/d9fb1038-bba0-11e8-b2b9-ac1f6b6435d0_cell_14_blue.png', '../input/image_subset/cell/d9fb1038-bba0-11e8-b2b9-ac1f6b6435d0_cell_14_yellow.png']\n",
      "(316, 285, 12)\n",
      "['../input/image_subset/cell/ae55b81a-bb9c-11e8-b2b9-ac1f6b6435d0_cell_5_red.png', '../input/image_subset/cell/ae55b81a-bb9c-11e8-b2b9-ac1f6b6435d0_cell_5_green.png', '../input/image_subset/cell/ae55b81a-bb9c-11e8-b2b9-ac1f6b6435d0_cell_5_blue.png', '../input/image_subset/cell/ae55b81a-bb9c-11e8-b2b9-ac1f6b6435d0_cell_5_yellow.png']\n",
      "(239, 276, 12)\n",
      "['../input/image_subset/cell/6658d460-bbb6-11e8-b2ba-ac1f6b6435d0_cell_11_red.png', '../input/image_subset/cell/6658d460-bbb6-11e8-b2ba-ac1f6b6435d0_cell_11_green.png', '../input/image_subset/cell/6658d460-bbb6-11e8-b2ba-ac1f6b6435d0_cell_11_blue.png', '../input/image_subset/cell/6658d460-bbb6-11e8-b2ba-ac1f6b6435d0_cell_11_yellow.png']\n",
      "(700, 195, 12)\n",
      "['../input/image_subset/cell/673febe2-bbad-11e8-b2ba-ac1f6b6435d0_cell_1_red.png', '../input/image_subset/cell/673febe2-bbad-11e8-b2ba-ac1f6b6435d0_cell_1_green.png', '../input/image_subset/cell/673febe2-bbad-11e8-b2ba-ac1f6b6435d0_cell_1_blue.png', '../input/image_subset/cell/673febe2-bbad-11e8-b2ba-ac1f6b6435d0_cell_1_yellow.png']\n",
      "(650, 410, 12)\n",
      "['../input/image_subset/cell/027c2e8a-bb9b-11e8-b2b9-ac1f6b6435d0_cell_7_red.png', '../input/image_subset/cell/027c2e8a-bb9b-11e8-b2b9-ac1f6b6435d0_cell_7_green.png', '../input/image_subset/cell/027c2e8a-bb9b-11e8-b2b9-ac1f6b6435d0_cell_7_blue.png', '../input/image_subset/cell/027c2e8a-bb9b-11e8-b2b9-ac1f6b6435d0_cell_7_yellow.png']\n",
      "(874, 768, 12)\n",
      "['../input/image_subset/cell/e852144e-bbad-11e8-b2ba-ac1f6b6435d0_cell_41_red.png', '../input/image_subset/cell/e852144e-bbad-11e8-b2ba-ac1f6b6435d0_cell_41_green.png', '../input/image_subset/cell/e852144e-bbad-11e8-b2ba-ac1f6b6435d0_cell_41_blue.png', '../input/image_subset/cell/e852144e-bbad-11e8-b2ba-ac1f6b6435d0_cell_41_yellow.png']\n",
      "(145, 326, 12)\n",
      "['../input/image_subset/cell/b0780108-bbaa-11e8-b2ba-ac1f6b6435d0_cell_24_red.png', '../input/image_subset/cell/b0780108-bbaa-11e8-b2ba-ac1f6b6435d0_cell_24_green.png', '../input/image_subset/cell/b0780108-bbaa-11e8-b2ba-ac1f6b6435d0_cell_24_blue.png', '../input/image_subset/cell/b0780108-bbaa-11e8-b2ba-ac1f6b6435d0_cell_24_yellow.png']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(446, 485, 12)\n",
      "['../input/image_subset/cell/1e312e66-bba8-11e8-b2ba-ac1f6b6435d0_cell_13_red.png', '../input/image_subset/cell/1e312e66-bba8-11e8-b2ba-ac1f6b6435d0_cell_13_green.png', '../input/image_subset/cell/1e312e66-bba8-11e8-b2ba-ac1f6b6435d0_cell_13_blue.png', '../input/image_subset/cell/1e312e66-bba8-11e8-b2ba-ac1f6b6435d0_cell_13_yellow.png']\n",
      "(188, 200, 12)\n",
      "['../input/image_subset/cell/ca30264c-bba1-11e8-b2b9-ac1f6b6435d0_cell_6_red.png', '../input/image_subset/cell/ca30264c-bba1-11e8-b2b9-ac1f6b6435d0_cell_6_green.png', '../input/image_subset/cell/ca30264c-bba1-11e8-b2b9-ac1f6b6435d0_cell_6_blue.png', '../input/image_subset/cell/ca30264c-bba1-11e8-b2b9-ac1f6b6435d0_cell_6_yellow.png']\n",
      "(716, 389, 12)\n",
      "['../input/image_subset/cell/486e9f2e-bb9e-11e8-b2b9-ac1f6b6435d0_cell_10_red.png', '../input/image_subset/cell/486e9f2e-bb9e-11e8-b2b9-ac1f6b6435d0_cell_10_green.png', '../input/image_subset/cell/486e9f2e-bb9e-11e8-b2b9-ac1f6b6435d0_cell_10_blue.png', '../input/image_subset/cell/486e9f2e-bb9e-11e8-b2b9-ac1f6b6435d0_cell_10_yellow.png']\n",
      "(576, 424, 12)\n",
      "['../input/image_subset/cell/0f260ff4-bbb2-11e8-b2ba-ac1f6b6435d0_cell_5_red.png', '../input/image_subset/cell/0f260ff4-bbb2-11e8-b2ba-ac1f6b6435d0_cell_5_green.png', '../input/image_subset/cell/0f260ff4-bbb2-11e8-b2ba-ac1f6b6435d0_cell_5_blue.png', '../input/image_subset/cell/0f260ff4-bbb2-11e8-b2ba-ac1f6b6435d0_cell_5_yellow.png']\n",
      "(639, 601, 12)\n",
      "['../input/image_subset/cell/f134134e-bbaf-11e8-b2ba-ac1f6b6435d0_cell_28_red.png', '../input/image_subset/cell/f134134e-bbaf-11e8-b2ba-ac1f6b6435d0_cell_28_green.png', '../input/image_subset/cell/f134134e-bbaf-11e8-b2ba-ac1f6b6435d0_cell_28_blue.png', '../input/image_subset/cell/f134134e-bbaf-11e8-b2ba-ac1f6b6435d0_cell_28_yellow.png']\n",
      "(597, 366, 12)\n",
      "['../input/image_subset/cell/ff015462-bba2-11e8-b2b9-ac1f6b6435d0_cell_7_red.png', '../input/image_subset/cell/ff015462-bba2-11e8-b2b9-ac1f6b6435d0_cell_7_green.png', '../input/image_subset/cell/ff015462-bba2-11e8-b2b9-ac1f6b6435d0_cell_7_blue.png', '../input/image_subset/cell/ff015462-bba2-11e8-b2b9-ac1f6b6435d0_cell_7_yellow.png']\n",
      "(552, 940, 12)\n",
      "['../input/image_subset/cell/2ad72f26-bba8-11e8-b2ba-ac1f6b6435d0_cell_18_red.png', '../input/image_subset/cell/2ad72f26-bba8-11e8-b2ba-ac1f6b6435d0_cell_18_green.png', '../input/image_subset/cell/2ad72f26-bba8-11e8-b2ba-ac1f6b6435d0_cell_18_blue.png', '../input/image_subset/cell/2ad72f26-bba8-11e8-b2ba-ac1f6b6435d0_cell_18_yellow.png']\n",
      "(268, 134, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                    | 0/4862 [00:08<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 7, 7], expected input[32, 12, 512, 512] to have 3 channels, but got 12 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-aa45435dce55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# Model training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     model.fit(\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# dataset inits are overriden in the model class above\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mvalid_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# otherwise tez breaks for me when it tries to do this itself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cell\\lib\\site-packages\\tez\\model\\model.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, train_dataset, valid_dataset, train_sampler, valid_sampler, device, epochs, train_bs, valid_bs, n_jobs, callbacks, fp16, train_collate_fn, valid_collate_fn)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menums\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrainingState\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEPOCH_START\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menums\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrainingState\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN_EPOCH_START\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m             \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_one_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menums\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrainingState\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN_EPOCH_END\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cell\\lib\\site-packages\\tez\\model\\model.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[1;34m(self, data_loader)\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mb_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtk0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menums\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrainingState\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN_STEP_START\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_one_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menums\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrainingState\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN_STEP_END\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cell\\lib\\site-packages\\tez\\model\\model.py\u001b[0m in \u001b[0;36mtrain_one_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain_one_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cell\\lib\\site-packages\\tez\\model\\model.py\u001b[0m in \u001b[0;36mmodel_fn\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    150\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m             \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cell\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-06685c3d799e>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, image, target)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# Extracts 512x1 feature vector from pretrained resnet18 conv layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvolutions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;31m# Fully connected dense layer to 19 class output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cell\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cell\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cell\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cell\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cell\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    393\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 395\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    396\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 7, 7], expected input[32, 12, 512, 512] to have 3 channels, but got 12 channels instead"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for i, split in enumerate(splits):\n",
    "    print('Fold {}'.format(i))\n",
    "    # generate dataloaders for each fold\n",
    "    train_dl, val_dl = get_split_dataloaders(split, batch_size)\n",
    "                                             #train_aug, valid_aug)\n",
    "    # Init model \n",
    "    model = ResNet18(train_dl,\n",
    "                     val_dl, \n",
    "                     batch_size=16, \n",
    "                     pretrained=False)\n",
    "\n",
    "    # Early stopping\n",
    "    es = EarlyStopping(\n",
    "        monitor='valid_loss',\n",
    "        model_path='model.bin',\n",
    "        patience=3,\n",
    "        mode='min',\n",
    "    )\n",
    "\n",
    "    # Model training\n",
    "    model.fit(\n",
    "        train_dataset=None,  # dataset inits are overriden in the model class above\n",
    "        valid_dataset=None,  # otherwise tez breaks for me when it tries to do this itself\n",
    "        train_bs=16,\n",
    "        device='cuda', \n",
    "        callbacks=[es],\n",
    "        epochs=1\n",
    "    )\n",
    "\n",
    "    # Save model (with optimizer and scheduler for future usage)\n",
    "    model.save('final_model_split_{}.bin'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-activation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cell]",
   "language": "python",
   "name": "conda-env-cell-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
